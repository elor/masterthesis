\section{Parsivald-Modell}
\label{sec:parsivald}

\subsection{Beschreibung}

Parsivald entstand als damals namenloses Resultat meiner Bachelorarbeit\cite{lorenz_entwicklung_2012} am Fraunhofer ENAS in Chemnitz mit dem Ziel der Atom\-lagen\-abscheidungs-Simulationen mit Beschränkung auf atomistische Simulation von Metall\-oxid-ALD mittels MEAM-Potentialen.
Im Hintergrund werden per Host-Worker-Mechanismus aus dem Simulationsraum wiederholt Simulationseinheiten extrahiert, mit MD-Methoden simuliert und zurückführt (Abbildung \ref{fig:parsivald-schema}).
Dafür wird LAMMPS neben einer eigenen KMC-Bibliothek, Importer und Exporter verschiedener Dateiformate, Kommandozeilenargumente und Konfigurationsdateien und einer MPI-orthogonalen Kommunikationsschnittstelle in die mehrteilige Software integriert.

Abbildung \ref{fig:parsivald-stephierarchy} stellt die aktuelle Funktionsweise des Parsivald-Modelles vor, die im wesentlichen aus Vor- und Nachbereitung sowie einer Prozessschleife besteht.
Innerhalb der Hauptschleife werden nacheinander einer oder mehrere Prozessschritte simuliert, die jeweils ein oder mehrere KMC-Ereignisarten umfassen können.
Das beinhaltet Relaxationen, Atom- oder Molekülabscheidungen, die jeweils mittels Molekulardynamik simuliert werden.
KMC-Ereignisse sind dabei durch die allgemeinen Prozesseigenschaften auf eine Oberflächenschicht begrenzt, wo sie jeweils innerhalb eines Kastens beschränker Größe, der MD-Box, wirken (Abbildung \ref{fig:parsivald-schema}).
Das setzt zwar diffusionsarme Prozesse voraus, ermöglicht dafür jedoch die effiziente Beschreibung von Atomlagenabscheidungen.

\begin{figure}
  \centering
  \def\svgwidth{\textwidth}
  \input{img/parsivald-schema-flat.pdf_tex}
  \caption[Parsivald-Schema]{
    Auswahl und Durchführung eines Oberflächenereignisses, verteilt auf KMC und MD.
  }
  \label{fig:parsivald-schema}
\end{figure}

\begin{figure}
  \centering
  \def\svgwidth{\textwidth}
  \input{img/parsivald-stephierarchy.pdf_tex}
  \caption[Parsivald-Funktionsweise asd]{
    Schema der Funktionsweise des Parsivald-Modelles für allgemeine Prozessarten.
    \\
    Event-Typen unterscheiden sich durch Art der MD-Simulation (Relaxation, Abscheidung) oder Art des Precursors.
    Siehe auch Abbildung \ref{fig:parsivald-modes}.
  }
  \label{fig:parsivald-stephierarchy}
\end{figure}

Als Eingaben der Simulation dienen das Substrat, eine valide Prozesskonfiguration sowie MD-Befehlslisten mit Platzhaltern (MD-Maske) samt Potentialparametrisierung.
Die Konfiguration legt Umgebungseigenschaften, Abscheidungsart (Abbildung \ref{fig:parsivald-modes}), Größe des Simulations- und MD-Raumes und Laufzeitbedingungen des Hostprogrammes fest.
Die Ausgabe erfolgt kontinuierlich in Ereignis-Logs sowie nach jedem Prozess-Schritt in Form aller Atompositionen, aus denen die Schichteigenschaften bestimmt werden können.
Optional lassen sich auch einzelne Events zur späteren Neuberechnung zwecks Fehlersuche speichern.

Eine typische ALD-Simulation folgt also folgendem Schema:
Zuerst wird das Substrat aus einer Datei gelesen und periodisch auf die Größe des Simulationsraums erweitert.
Pro Halbzyklus wird eine Ereignismaske vorbereitet, die die physikalischen und numerischen Eigenschaften inklusive der notwendigen MD-Befehle zwischenspeichert.
Anschließend beginnt die Hauptschleife mit Schritt 1, der den ersten Halbzyklus darstellt.
Es werden mögliche Reaktionsorte auf der Oberfläche gesucht und per KMC-Algorithmus Ereignisse ausgewählt und in MD-Prozessen simuliert.
Wird das Timeout des Schrittes erreicht, startet zyklisch der nächste Schritt, wodurch auch der gesamte Zyklus neu starten kann.
Nach jedem Schritt werden auf Wunsch alle Atome in einem atomistischen Speicherformat zur weiteren Analyse auf die Festplatte geschrieben.
Nach einer vorgegebenen Anzahl an Zyklen endet die Abscheidungssimulation erfolgreich.

Mit diesem Schema lassen sich auch CVD- und PVD-Prozesse simulieren, indem man sich auf einen Schritt pro Zyklus beschränkt und im CVD-Fall mehrere Ereignisarten gleichzeitig nutzt.
Der Zyklus-Timeout kann dann zur Kontrolle der Ausgabefrequenz genutzt werden.
Für ALD und CVD lässt sich die Abscheidungsrate aus der Reaktionskinetik abschätzen, während man für PVD-Simulationen die Rate entsprechend der Prozessparameter setzen kann.

\begin{figure}
  \captionsetup[subfigure]{singlelinecheck=false}
  \begin{subfigure}[t]{5.7cm}
    \def\svgwidth{\textwidth}
    \input{img/parsivald-modes-ald.pdf_tex}
    \subcaption{ALD-Modus}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{4.7cm}
    \def\svgwidth{\textwidth}
    \input{img/parsivald-modes-cvd.pdf_tex}
    \subcaption{CVD-Modus}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{3cm}
    \def\svgwidth{\textwidth}
    \input{img/parsivald-modes-pvd.pdf_tex}
    \subcaption{PVD-Modus}
  \end{subfigure}
  \caption[Parsivald-Modi]{
    Verschiedene Parsivald-Modi im Prozesszyklus.
    Atomistische und statistische Ausgaben erfolgen nach jedem Schritt.
    ALD nutzt einen Schritt pro Halbzyklus.
  }
  \label{fig:parsivald-modes}
\end{figure}

\subsection{Fortschritt im Rahmen der Masterarbeit}

Seit Beginn dieser Arbeit wurde das Parsivald-Modell von ALD- auf PVD- und CVD-Simu\-lationen erweitert.
Dies wurde durch zentral verwaltete Konfigurationen ermöglicht, in denen Konfigurations-, Substrat- und Reaktionsdateien (MD-Masken) verschiedener Prozesse verwaltet werden.
Durch die Prozess-Konfigurationen sind schnelle Prozessentwicklungszyklen möglich, die durch semi-automatische Generierung unterstützt wird.
Auch eine vollautomatische Prozesserforschung hinsichtlich passender Vorgaben ist mittels des Konfigurationsmechanismus' denkbar.

Zudem wurde das Host-Worker-System von einzelnen Workern auf Workerpools erweitert, die von einem Hauptprozess pro Rechenknoten verwaltet werden.
Dies verringert den \todo{grausames Wort}Startup-Overhead einzelner Worker zugunsten der Gesamtleistung des Pools.
In den Hostprozess ist ein ein optionaler, interner Workerpool eingebettet, der durch veränderte Parallelisierungsmechanismen Fehler der MD-Bibliothek sicher abfangen kann (Abschnitt \ref{lammpssucks}).


\subsection{Probleme mit LAMMPS}
\label{lammpssucks}

Der hervorragende LAMMPS-Code wurde als MD-Bibliothek genutzt, da er gut optimiert und auf Funktionsebene in eigene Programme integrierbar ist sowie ReaxFF-Potentiale im Standardformat unterstützt.
Damit ist LAMMPS die einzige Wahl, allerdings stößt man bei der Nutzung auf verschiedene Probleme.

Ein Hauptproblem besteht in der Interaktion mit MPI.
So liegt nahe, für hochparallele Systeme wie Parsivald auf bestehende MPI-Bibliotheken zu setzen, um Tasks vom Host an seine Worker zu vermitteln.
LAMMPS nutzt jedoch intern globale MPI-Calls, auch wenn nur kleine Systeme untersucht werden oder explizit auf MPI verzichtet wird\todo{Beleg!}.
Dieses Problem lässt sich durch fork-Prozesse oder proprietäre Patches umgehen, kann aber keine Dauerlösung für so komplexe Software wie LAMMPS sein.
Zwar lässt sich LAMMPS auch mit einem MPI-Stub kompilieren, jedoch führt dieser durch kollidierende Namen globaler Funktionen der notwendigerweise dynamisch verlinkten MPI-Bibliotheken zu Problemen.
Durch \todo{unverhältnismäßi\-g? wirklich?}unverhältnismäßigen Aufwand ließe sich auch dieses Problem umgehen, allerdings stößt man auch dann auf folgendes Problem.

LAMMPS ruft nach jedem Berechnungsfehler \texttt{exit} auf, wodurch der aktuelle Prozess ohne Möglichkeit der Fehlerkorrektur abgebrochen und die Kommunikation mit dem Hostprozess hart unterbrochen wird.
Geschieht dies in einer MPI-Umgebung, werden alle beteiligten Prozesse unter Annahme eines kritischen Fehlers beendet.
LAMMPS ruft jedoch mit hoher Regelmäßigkeit Fehler hervor, beispielsweise bei unerwartetem Übertreten von Systemgrenzen, hohen Teilchengeschwindigkeiten oder Ausnahmesituationen durch fehlgeformte Potentiale.
Die Lösung gestaltete sich in der Gestaltung eines eigenen Host-Worker-Systemes, das LAMMPS in gekapselten Prozessen lokal oder auf entfernten Knoten laufen lässt und nur die Anfangsbedingungen und Ergebnisse kommuniziert.
Damit sind bei Abbrüchen die genauen Gründe unbekannt: Aus Sicht des Hostprozesses besteht ein Verbindungsabbruch.
Die einzige Möglichkeit besteht darin, die MD-Simulation erneut zu starten und bei wiederholtem Abbruch von unzureichenden Anfangsbedingungen auszugehen.

Darüber hinaus fallen einige LAMMPS-Rechnungen unerwartet in eine Dauerschleife, die sich aufgrund der blockierenden Architektur der Bibliothek nicht ohne separaten Überwachungsprozess diagnostizieren oder beenden lässt.
Hier besteht die Lösung schlicht in einem Timeout für jeden verbundenen Worker, der durch hostseitigen Verbindungsabbruch realisiert wird.
Nach einer unbestimmten Zeit nach dem Verbindungsabbruch startet sich der Worker anschließend neu.
Im Worst-Case-Szenario blockieren alle Worker für unbestimmte Zeit, wodurch die Simulation signifikant ausgebremst wird, jedoch weiterhin stabil läuft.

Vom Hostprozess werden für statistische Auswertungen Zähler der erfolgreichen und fehlerhaften MD-Simulationen geführt, mit denen Fehlerraten mit Prozess- und Struktureigenschaften korreliert werden können.
Eine Anwendung davon befindet sich in Abschnitt \ref{coppersimulation}.

\subsection{something else}

asd

\todo{wozu?}Dazu wird der gesamte Wachstumsprozess in das Auftreffen eines Precursormoleküles oder Atomes und seine Reaktion beziehungsweise physikalische Anlagerung an der Oberfläche getrennt.
Für alle weiteren Prozesse wurden folgende Annahmen getroffen:

\begin{enumerate}
\item Alle Reaktionen finden auf der Oberfläche statt
\item Reaktionen sind zeitlich und räumlich getrennt
\item Oberflächendiffusion ist vernachlässigbar
\item Bulkdiffusion ist vernachlässigbar
\end{enumerate}

Reaktionen in der Gasphase sind also ebenso wenig darstellbar wie diffusionsgestützte Prozesse.
Ohne diese notwendigen Annahmen käme das Parsival-Modell einer reinen Molekular\-dynamik-Simu\-lation gleich.
Stattdessen werden Reaktionen auf folgende Weise modelliert:

Auf der Oberfläche des gesamten Simulationsraumes wird zuerst nach einem Ort für eine Precursorreaktion gesucht.
Dessen Auswahl kann rein zufällig sein, oder aber von der lokalen Nachbarschaft abhängen, beispielsweise über molekulare Gruppen oder Atomdichten.
Nach dessen Auswahl extrahiert man alle Atome in der unmittelbaren Nachbarschaft und erstellt aus ihnen eine Reaktionszelle für MD-Simulationen.
Diese Zelle enthält einen Rand fester Atome, dessen Größe wie auch die Größe der Zelle selbst von der Reichweite der genutzten MD-Potentiale abhängt.
In diese Zelle fügt man je nach Simulationsart, gewünschter Genauigkeit und Fähigkeit der Potentiale das Precursormolekül oder einzelne Atome daraus ein.
Nach einer erfolgreichen Reaktions- oder Relaxationssimulation fügt man die Atome der Reaktionszelle wieder in die Gesamtstruktur ein und wählt das nächste Oberflächenereignis aus.

Ergänzend lässt sich durch verschiedene Optimierungen die Laufzeit minimieren.
Beispielsweise kann man durch Parallelisierung des KMC-Teiles linearen Speedup \todo{Linearer Speedup?} erreichen, der nur durch die Größe des Raumes begrenzt ist.
Eine Parallelisierung der MD-Simulationen ist hingegen nicht sinnvoll, da bloß kleine Mengen von maximal einigen Tausend Atomen gleichzeitig simuliert werden.
Andererseits kann man durch oben diskutierte Datenstrukturen die Suche neuer KMC-Ereignisse sowie die Extraktion einer atomaren Nachbarschaft verkürzen.

\todo{fehlt noch was?}

\subsection{Einschränkungen}

Neben den durch die oben genannten Annahmen \todo{Referenz} eingeführten gibt es noch eine weitere Einschränkung.
So lassen sich nur Prozesse und Strukturen betrachten, die auch durch Molekulardynamik darstellbar sind, wobei sich sowohl das Bulk-Material als auch seine Oberfläche simulieren lassen müssen.
Im optimalen Fall stellen die genutzten MD-Potentiale auch noch Precursor-Oberflächen-Reaktionen verlässlich dar, so dass man auf weitere Annäherungen verzichten kann.
Darunter fallen die Reduktion des Precursors auf sein Zentralatom, explizite sterische Hinderung sowie eine ausführliche Vorauswahl der Reaktionsorte.

Falls sich der komplette Precursor oder seine Reaktion mit der Oberfläche nicht durch passende MD-Potentiale darstellen lässt, kann man auf die Abscheidung seines Zentralatomes zurück greifen.
Obwohl es sich um eine stark vereinfachte Methode einer chemischen Abscheidung handelt, lässt sich so wenigstens das Wachstum der Struktur beobachten.
Da sich so keine Hinweise auf zurück gebliebene Precursorliganden auf der Oberfläche finden, muss man zusätzlich die sterische Hinderung explizit modellieren, um freies Wachstum zu verhindern.

\missingfigure{steric-hindrance}

Sterische Hinderung verhindert normalerweise durch die lokale Oberflächendichte von Precursorliganden eine weitere Anlagerung von Precursormolekülen bei chemischen Abscheidungen, bis diese Liganden wieder von der Oberfläche entfernt wurden.
Als Ersatz wird eine kugelförmige Zone definiert, innerhalb derer weitere Precursorreaktionen auf der Oberfläche ausgeschlossen sind, bis der nächste Prozessschritt gestartet wurde.
Als entsprechender Parameter steht nun jedem möglichen Abscheidungsereignis der Radius dieser Kugel zur Verfügung.
Diese Methode ignoriert Feinheiten wie die genaue Anzahl an verbliebenen Precursorliganden, stellt jedoch eine gute\todo{wirklich gut?} Näherung dar.

\subsection{Implementierung}

Das Parsivald-Modell wurde ursprünglich im Rahmen meiner Bachelorarbeit \todo{Referenz} in Software umgesetzt.
Seither wurden verschiedene Funktionen ergänzt, wie beispielsweise Erleichterungen im Prozessdesign sowie der separate PVD-Modus.
Auch einfachere Möglichkeiten, eine bestimmte Zahl an Worker-Prozessen zu starten, vereinfacht die Nutzung der Software auf größeren Clustersystemen.

Grundlage der Implementierung bieten hauptsächlich die beiden Bibliotheken libenskmc sowie LAMMPS für KMC und MD, wobei libenskmc aus eigener Entwicklung stammt.
Man hätte an dieser Stelle auch die SPPARKS-Bibliothek für KMC-Simulationen einbinden können, die von den selben Autoren der LAMMPS-Bibliothek stammt.
Es hat sich jedoch heraus gestellt, dass SPPARKS nur unter großem Aufwand das Parsivald-Modell handhaben könnte, was vor allem in Hinblick auf häufige kritische Abbrüche der LAMMPS-Bibliothek nicht vertretbar war.

Auf eigenen Code wurde bei folgenden Systemen gesetzt:
Das Host-Worker-System mit angebundenen Netzwerk-Code musste geschrieben werden, da LAMMPS die Nutzung einer MPI-Bibliothek unterbindet.
Die Octrees zum räumlichen Binning wurden eng mit libenskmc verzahnt, weshalb auf eine externe Geometrie-Bibliothek verzichtet wurde.
DUMP-Substrate werden aufgrund der klaren Format-Definition durch eigenen Code eingelesen.
Ebenso erfolgt die Ausgabe der .xyz-Dateien über eigenen Code.
Im Gegensatz dazu werden .lmp-Substrate jedoch über lmpio eingelesen, einer IO-Bibliothek für .lmp-Dateien, die im Hintergrund LAMMPS nutzt, um Kompatibilität mit der Vielzahl an uneindeutigen Atomeigenschaften zu garantieren.
Um Konformität zwischen Kommandozeilenargumenten und Konfigurationsdateien zu garantieren, wurde auf die ArgumentParser-Bibliothek zurück gegriffen, die aus eigener Feder entstammt und in weiteren externen Projekten Verwendung findet.
Die Abhängigkeitsauflösung bei KMC-Ereignissen entstammt ebenfalls der eigenen Feder, ebenso wie der sonstige Binding Code zwischen LAMMPS und libenskmc, inklusive beider Parallelisierung-Codes (Threads und losgelöste Unterprozesse).

