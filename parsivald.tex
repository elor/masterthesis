\section{Parsivald-Modell}
\label{sec:parsivald}

\todo[inline]{Soll ich die Zellen beschrieben?}

\subsection{Beschreibung}

Parsivald entstand als damals namenloses Resultat meiner Bachelorarbeit\cite{lorenz_entwicklung_2012} am Fraunhofer ENAS in Chemnitz mit dem Ziel der Simulation von Atom\-lagen\-abscheidungs-Prozessen.
Das Programm war beschränkt auf die atomistische Simulation von Metall\-oxid-ALD mittels MEAM-Potentialen.
Im Hintergrund werden per Host-Worker-Mechanismus aus dem Simulationsraum wiederholt Simulationseinheiten extrahiert, mit MD-Methoden simuliert und zurückführt (Abbildung \ref{fig:parsivald-schema}).
Dafür wird LAMMPS neben einer eigenen KMC-Bibliothek, Importer und Exporter verschiedener Dateiformate, Kommandozeilenargumente und Konfigurationsdateien und einer MPI-orthogonalen Kommunikationsschnittstelle in die mehrteilige Software integriert.

Abbildung \ref{fig:parsivald-stephierarchy} stellt die aktuelle Funktionsweise des Parsivald-Modelles vor, die im wesentlichen aus Vor- und Nachbereitung sowie einer Prozessschleife besteht.
Innerhalb der Hauptschleife werden nacheinander einer oder mehrere Prozessschritte simuliert, die jeweils ein oder mehrere KMC-Ereignisarten umfassen können.
Das beinhaltet Relaxationen, Atom- oder Molekülabscheidungen, die jeweils mittels Molekulardynamik simuliert werden.
KMC-Ereignisse sind dabei durch die allgemeinen Prozesseigenschaften auf eine Oberflächenschicht begrenzt, wo sie jeweils innerhalb eines Kastens beschränker Größe, der MD-Box, wirken (Abbildung \ref{fig:parsivald-schema}).
Das setzt zwar diffusionsarme Prozesse voraus, ermöglicht dafür jedoch die effiziente Beschreibung von Atomlagenabscheidungen.

\begin{figure}
  \centering
  \def\svgwidth{\textwidth}
  \input{img/parsivald-schema-flat.pdf_tex}
  \caption[Parsivald-Schema]{
    Auswahl und Durchführung eines Oberflächenereignisses, verteilt auf KMC und MD.
  }
  \label{fig:parsivald-schema}
\end{figure}

\begin{figure}
  \centering
  \def\svgwidth{\textwidth}
  \input{img/parsivald-stephierarchy.pdf_tex}
  \caption[Parsivald-Funktionsweise asd]{
    Schema der Funktionsweise des Parsivald-Modelles für allgemeine Prozessarten.
    \\
    Event-Typen unterscheiden sich durch Art der MD-Simulation (Relaxation, Abscheidung) oder Art des Precursors.
    Siehe auch Abbildung \ref{fig:parsivald-modes}.
  }
  \label{fig:parsivald-stephierarchy}
\end{figure}

Als Eingaben der Simulation dienen das Substrat, eine valide Prozesskonfiguration sowie MD-Befehlslisten mit Platzhaltern (MD-Maske) samt Potentialparametrisierung.
Die Konfiguration legt Umgebungseigenschaften, Abscheidungsart (Abbildung \ref{fig:parsivald-modes}), Größe des Simulations- und MD-Raumes und Laufzeitbedingungen des Hostprogrammes fest.
Die Ausgabe erfolgt kontinuierlich in Ereignis-Logs sowie nach jedem Prozess-Schritt in Form aller Atompositionen, aus denen die Schichteigenschaften bestimmt werden können.
Optional lassen sich auch einzelne Events zur späteren Neuberechnung zwecks Fehlersuche speichern.

Eine typische ALD-Simulation folgt also folgendem Schema:
Zuerst wird das Substrat aus einer Datei gelesen und periodisch auf die Größe des Simulationsraums erweitert.
Pro Halbzyklus wird eine Ereignismaske vorbereitet, die die physikalischen und numerischen Eigenschaften inklusive der notwendigen MD-Befehle zwischenspeichert.
Anschließend beginnt die Hauptschleife mit Schritt 1, der den ersten Halbzyklus darstellt.
Es werden mögliche Reaktionsorte auf der Oberfläche gesucht und per KMC-Algorithmus Ereignisse ausgewählt und in MD-Prozessen simuliert.
Wird das Timeout des Schrittes erreicht, startet zyklisch der nächste Schritt, wodurch auch der gesamte Zyklus neu starten kann.
Nach jedem Schritt werden auf Wunsch alle Atome in einem atomistischen Speicherformat zur weiteren Analyse auf die Festplatte geschrieben.
Nach einer vorgegebenen Anzahl an Zyklen endet die Abscheidungssimulation erfolgreich.

Mit diesem Schema lassen sich auch CVD- und PVD-Prozesse simulieren, indem man sich auf einen Schritt pro Zyklus beschränkt und im CVD-Fall mehrere Ereignisarten gleichzeitig nutzt.
Der Zyklus-Timeout kann dann zur Kontrolle der Ausgabefrequenz genutzt werden.
Für ALD und CVD lässt sich die Abscheidungsrate aus der Reaktionskinetik abschätzen, während man für PVD-Simulationen die Rate entsprechend der Prozessparameter setzen kann.

\begin{figure}
  \captionsetup[subfigure]{singlelinecheck=false}
  \begin{subfigure}[t]{5.7cm}
    \def\svgwidth{\textwidth}
    \input{img/parsivald-modes-ald.pdf_tex}
    \subcaption{ALD-Modus}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{4.7cm}
    \def\svgwidth{\textwidth}
    \input{img/parsivald-modes-cvd.pdf_tex}
    \subcaption{CVD-Modus}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{3cm}
    \def\svgwidth{\textwidth}
    \input{img/parsivald-modes-pvd.pdf_tex}
    \subcaption{PVD-Modus}
  \end{subfigure}
  \caption[Parsivald-Modi]{
    Verschiedene Parsivald-Modi im Prozesszyklus.
    Atomistische und statistische Ausgaben erfolgen nach jedem Schritt.
    ALD nutzt einen Schritt pro Halbzyklus.
  }
  \label{fig:parsivald-modes}
\end{figure}

\subsection{Annahmen und Einschränkungen}

Trotz der Erweiterung auf CVD und PVD bleibt das Parsivald-Modell auf eine spezielle Klasse von Problemen begrenzt.
So werden das Auftreffen des Precursors und seine Reaktion mit der Oberfläche \todo{naja, so richtig auch nicht}separat betrachtet, auf eine atomistische Modellierung der Gasphase jedoch verzichtet, wozu folgende Annahmen getroffen werden:

\begin{enumerate}
\item Alle Reaktionen finden auf der Oberfläche statt
\item Reaktionen sind zeitlich und räumlich getrennt
\item Oberflächendiffusion ist vernachlässigbar
\item Bulkdiffusion ist vernachlässigbar
\end{enumerate}

Unter Vernachlässigung einzelner Annahmen reduzierte sich das Parsivald-Modell auf reine Molekulardynamik und wäre somit auf wenige hunderttausend Teilchen begrenzt.
Dennoch bleiben Genauigkeit und Umfang der Ergebnisse auf molekulardynamische Methoden begrenzt.
Somit lassen sich nur Potentialdateien nutzen, die sowohl Bulks als auch Oberflächen simulieren können.
Werden zusätzlich die Simulation von Molekülen und deren Reaktionen mit der Oberfläche unterstützt, lassen sich die Precursor \todo{theoretisch?}theoretisch direkt betrachten.
Andernfalls muss der Precursor auf das abzuscheidende Zentralatom reduziert und die sterische Hinderung durch seine Liganden sowie die Suche nach Abscheidungsorten mangels \todo{Begriff}Oberflächenliganden separat behandelt werden.

\subsection{Fortschritt im Rahmen der Masterarbeit}

\todo[inline]{das ist wohl zu viel}
Seit Beginn dieser Arbeit wurde das Parsivald-Modell und seine Implementierung um PVD- und CVD-Modi (Abbildung \ref{fig:parsivald-modes}), ein allgemeines Konfigurationsformat\todo{Anhang?}, MD-Befehle mit Platzhaltern (MD-Masken), Unterstützung verschiedener atomistischer Dateiformate, Einbettung der LAMMPS-Umgebungs\-variablen zur Potentialsuche, globale und lokale Suchpfade für alle Eingabedateien, eventgebundene sterische Hinderung zum Zweck der CVD-Simu\-lation, einen internen und beliebig viele externe Workerpools, ein standardisiertes Buildsystem sowie eine Vielzahl externer \todo{ref oder anhang}Werkzeuge zur Prozessvorbereitung und -analyse erweitert.

Diese Änderungen ermöglichen eine einfachere Vorbereitung, Simulation und Erforschung verschiedener Prozesse und Analyse der Ergebnisse.
Auch eine automatisierte Prozesserforschung hinsichtlich passender Vorgaben ist mittels des Konfigurationsmechanismus' denkbar.
Zusätzlicher Aufwand musste beim Einkapseln der MD-Bibliothek LAMMPS betrieben werden (Abschnitt \ref{lammpssucks}).

\subsection{Probleme mit LAMMPS}
\label{lammpssucks}

Der hervorragende LAMMPS-Code wurde als MD-Bibliothek genutzt, da er gut optimiert und auf Funktionsebene in eigene Programme integrierbar ist sowie ReaxFF-Potentiale im Standardformat unterstützt.
Damit ist LAMMPS die einzige Wahl, allerdings stößt man bei der Nutzung auf verschiedene Probleme.

Ein Hauptproblem besteht in der Interaktion mit MPI-Bibliotheken.
So liegt nahe, für hochparallele Systeme wie Parsivald auf MPI zu setzen, um Tasks vom Host an seine Worker zu vermitteln.
LAMMPS nutzt jedoch intern globale MPI-Calls, auch wenn nur kleine Systeme untersucht werden oder explizit auf MPI verzichtet wird\todo{Beleg!}.
Dieses Problem lässt sich durch fork-Prozesse oder proprietäre Patches umgehen, ist aber keine zufriedenstellende, sichere Lösung für umfangreiche Software.
Zwar lässt sich LAMMPS auch mit einem MPI-Stub kompilieren, jedoch führt dieser durch kollidierende Namen globaler Funktionen der notwendigerweise dynamisch verlinkten MPI-Bibliotheken unweigerlich zu Problemen.
Auch nach Lösung dieses Problemes käme MPI in Konflikt mit dem nächsten Problem.

LAMMPS ruft nach jedem Berechnungsfehler \texttt{exit} auf, wodurch der aktuelle Prozess ohne Möglichkeit der Fehlerkorrektur beendet und die Kommunikation mit dem Hostprozess hart unterbrochen wird.
Geschieht dies in einem MPI-Prozess, werden alle beteiligten Prozesse auf allen Knoten unter Annahme eines kritischen Fehlers beendet.
Unerwartetes Übertreten von Systemgrenzen, hohen Teilchengeschwindigkeiten oder Ausnahmesituationen durch fehlgeformte Potentiale verursachen nun regelmäßig und unvorhergesehen \texttt{exit}-Calls und beenden ungewollt die gesamte Simulation.
Eine mögliche Lösung wäre die Nutzung von Prozess-Forks, die vom eigentlichen MPI-Prozess überwacht werden, wie es in Workerpool der Fall ist.

Darüber hinaus fallen einige Rechnungen unerwartet in eine Dauerschleife innerhalb der LAMMPS-Bibliothek, die sich aufgrund deren blockierender Architektur nicht ohne separaten Überwachungsprozess diagnostizieren oder beenden lässt.
Hier besteht die Lösung schlicht in einem hostseitigen Timeout der Verbindung zu jedem verbundenen Worker.
Eine unbestimmte Zeit nach dem Verbindungsabbruch startet sich der Worker selbsttätig neu.
Im Worst-Case-Szenario endet ein Großteil der Worker in Dauerschleifen, wodurch die Simulation signifikant ausgebremst wird, jedoch weiterhin stabil läuft.

Als Lösung der vorgestellten Probleme wurde ein eigenes Host-Worker-System gestaltet, das LAMMPS in gekapselten Prozessen lokal oder auf entfernten Knoten laufen lässt, aber nur die Anfangsbedingungen und Ergebnisse kommuniziert.
Worker werden in Pools organisiert, sind ansonsten aber unabhängig, was sich in der direkten Nutzung des Netzwerkstacks ohne Umweg über den Poolmanagement-Prozess zeigt.
Auf der einen Seite ist die Stabilität der Worker unabhängig von anderen Workern, andererseits bleiben die eigentlichen Fehlerursachen weiterhin verborgen.
Stattdessen werden fehlgeschlagene MD-Simulationen erneut gestartet und bei wiederholtem Berechnungsfehler als fehlgeschlagen markiert und verworfen.

Vom Hostprozess werden für statistische Auswertungen Zähler der erfolgreichen und fehlerhaften MD-Simulationen geführt, mit denen Fehlerraten mit Prozess- und Struktureigenschaften korreliert werden können.
Eine Anwendung davon befindet sich in Abbildung \ref{fig:copperparsivald}.
